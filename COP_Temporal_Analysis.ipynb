{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808ac66c",
   "metadata": {},
   "source": [
    "### Import packages / setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7eb69ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "\n",
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from my_api_keys import api_key, api_key_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a9f4f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the Tweepy API\n",
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "683e2b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Modifications to punctuation and sw lists\n",
    "\n",
    "punctuation = set(punctuation)\n",
    "punctuation.add(\"â€™\")\n",
    "\n",
    "sw2 = set(sw)\n",
    "addl = (\"|\",\"-\",\"/\",\"â€¢\",\"&\")\n",
    "sw2.update(addl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61943b",
   "metadata": {},
   "source": [
    "### Function for scraping tweets related to COP26\n",
    "#### The function scrapes 15k tweets per day and stores in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5ac51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapetweets(the_api, search_words, numtweets, numruns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try numruns times\n",
    "\n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['user_id','screen_name','description','location','friends_count',\n",
    "           'followers_count','totaltweets','date_created', 'tweet_id', 'retweetcount','full_text'])\n",
    "    \n",
    "    program_start = time.time()\n",
    "    for i in range(0, numruns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(the_api.search_tweets, \n",
    "                               q=search_words, \n",
    "                               lang=\"en\", \n",
    "                               tweet_mode='extended'\n",
    "                              ).items(numtweets)\n",
    "        \n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        \n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "    \n",
    "        for tweet in tweet_list:\n",
    "            userid = tweet.user.id\n",
    "            username = tweet.user.screen_name\n",
    "            description = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            date_created = tweet.created_at\n",
    "            tweet_id = tweet.id\n",
    "            retweetcount = tweet.retweet_count\n",
    "            full_text = tweet.full_text\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [userid, username, description, location, following, followers, totaltweets,\n",
    "                         date_created, tweet_id, retweetcount, full_text]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "                       \n",
    "            \n",
    "            \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "\n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "\n",
    "        time.sleep(920) #15 minute sleep time between runs\n",
    "\n",
    "    # Once all runs have completed, save them to a single csv file:\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.date.today().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_cop_tweets.csv'\n",
    "    \n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print(\"\\n\")\n",
    "    print(f'Scraping for {startdate} to {enddate} has completed!')\n",
    "    print('Total time taken to scrape is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8181fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 1 is 1\n",
      "time take for 1 run to complete is 0.01 mins\n",
      "\n",
      "\n",
      "Scraping for 2021-10-29 to 2021-10-30 has completed!\n",
      "Total time taken to scrape is 0.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "# COP26 was Oct 31 to Nov 12. Pull the conference, plus three days on either end. \n",
    "# So need Oct 28 to Nov 15. \n",
    "\n",
    "startdate = \"2021-10-30\"\n",
    "enddate = \"2021-10-31\"\n",
    "\n",
    "search_words = f'#cop26 OR #COPglasgow since:{startdate} until:{enddate} -filter:retweets'\n",
    "numtweets=2500\n",
    "numruns=6\n",
    "\n",
    "scrapetweets(api, search_words, numtweets, numruns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f5f07",
   "metadata": {},
   "source": [
    "###  Issues to address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd2509",
   "metadata": {},
   "source": [
    "deal with carriage returns\n",
    "\n",
    "tab-separator\n",
    "\n",
    "deal with hashtags separators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298fc430",
   "metadata": {},
   "source": [
    "####  Read daily tweets CSVs into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71ae473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.DataFrame(columns = ['user_id','screen_name','description','location','friends_count',\n",
    "           'followers_count','totaltweets','date_created', 'tweet_id', 'retweetcount','full_text'])\n",
    "\n",
    "file_location = \"/Users/natebender/Desktop/Repo/text-mining/datashare_AA_COP_tweets/data/\"\n",
    "files = sorted(os.listdir(file_location))\n",
    "for idx, file in enumerate(files):\n",
    "    \n",
    "    data = \"\".join([file_location,file])\n",
    "    datafile = pd.read_csv(data)\n",
    "\n",
    "    db = db.append(datafile,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ce513612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check desc stats on overall descriptions before splitting into groups\n",
    "def get_patterns(all_descriptions) :\n",
    "\n",
    "    all_desc = all_descriptions.dropna()\n",
    "    all_str = \" \".join(all_desc)    \n",
    "    clean = [w for w in all_str.split() if w.lower() not in sw2]\n",
    "    \n",
    "    # Calculate your statistics here\n",
    "    total_tokens = len(clean)\n",
    "    unique_tokens = len(set(clean))\n",
    "    clean_tok_len = [len(w) for w in clean]\n",
    "    avg_token_len = np.mean(clean_tok_len)\n",
    "    lex_diversity = len(set(clean))/len(clean)\n",
    "    top_10 = Counter(clean).most_common(10)\n",
    "    \n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':round(avg_token_len,2),\n",
    "               'lexical_diversity':round(lex_diversity,2),\n",
    "               'top_10':top_10}\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ea17d",
   "metadata": {},
   "source": [
    "### Desc stats on database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5cf7beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: 30,000 tweets\n",
      "With NAs removed: 27,108 descriptions\n",
      "Descriptive stats are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': 331578,\n",
       " 'unique_tokens': 20173,\n",
       " 'avg_token_length': 7.42,\n",
       " 'lexical_diversity': 0.06,\n",
       " 'top_10': [('climate', 1512),\n",
       "  ('ðŸ´\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f', 1428),\n",
       "  ('#COP26', 1380),\n",
       "  ('Climate', 1254),\n",
       "  ('Tweeting', 1194),\n",
       "  ('ðŸ¤–', 1128),\n",
       "  ('#UK', 1104),\n",
       "  ('aviation', 1104),\n",
       "  ('#COP26Glasgow', 1104),\n",
       "  ('activity', 1098)]}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NA values from description column\n",
    "all_desc = db.description.dropna()\n",
    "print(f'Database: {\"{:,}\".format(len(db.tweet_id))} tweets')\n",
    "print(f'With NAs removed: {\"{:,}\".format(len(all_desc))} descriptions')\n",
    "print(f'Descriptive stats are:')\n",
    "get_patterns(db.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b8c50",
   "metadata": {},
   "source": [
    "### Lexicon expansion function to analyze tweets by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6bb55124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexicon :\n",
    "    def __init__(self, corpus, search_condition, num_words, ratio_cutoff):\n",
    "        self.corpus = list(corpus)\n",
    "        self.search_condition = search_condition\n",
    "        self.num_words = num_words\n",
    "        self.ratio_cutoff = ratio_cutoff\n",
    "        \n",
    "    def parameters(self) :\n",
    "        print(f\"The corpus is {len(self.corpus)} total tweets\")\n",
    "        print(f\"The search string is {self.search_condition}\")\n",
    "        print(f\"Words must appear {self.ratio_cutoff} times in both corpora to be included in analysis\")\n",
    "        print(f\"Descriptive stats and comparisons for {self.num_words} words are returned\")\n",
    "        \n",
    "    def lex_expansion(self) :\n",
    "\n",
    "        # Set starting marker for keeping track of how long the function runs\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        group_1 = []\n",
    "\n",
    "        for string in self.corpus :\n",
    "            if self.search_condition.search(string) :\n",
    "                group_1.append(string)\n",
    "\n",
    "        group_1 = list(set(group_1))\n",
    "        group_2 = [string for string in self.corpus if string not in group_1]\n",
    "\n",
    "        group_1_tweets = (\"{:,}\".format(len(group_1)))\n",
    "        group_2_tweets = (\"{:,}\".format(len(group_2)))    \n",
    "\n",
    "        # Tokenize the two groups by first turning them each into single large strings\n",
    "        g1_str = \" \".join(group_1)    \n",
    "        g2_str = \" \".join(group_2)\n",
    "\n",
    "        # Then splitting each into lists of strings. Once we've created groups, we no longer need to know\n",
    "        # which words came from which specific tweets, as long as we keep track of which group the \n",
    "        # words come from.\n",
    "        # We also tokenize and normalize at this stage. We want to remove stopwords, but retain numeric as numbers\n",
    "        # could be used in interesting words, hashtags, or accounts in a corpus of tweets. \n",
    "        g1_uclean = [w for w in g1_str.split()]\n",
    "        g2_uclean = [w for w in g2_str.split()]\n",
    "\n",
    "        g1_clean = [w for w in g1_str.split() if w.lower() not in sw2]\n",
    "        g2_clean = [w for w in g2_str.split()if w.lower() not in sw2]\n",
    "\n",
    "        g1_len = len(g1_clean)\n",
    "        g2_len = len(g2_clean)\n",
    "\n",
    "        # SECT2: CREATE \"CUTOFF_LIST\" LIST OF WORDS THAT MEET CUTOFF RATIO\n",
    "        # Create Counter dictionary of each corpus, used for determining words that meet cutoff ratio\n",
    "        wcount_one = Counter(g1_clean)\n",
    "        wcount_two = Counter(g2_clean)\n",
    "\n",
    "        # Create list of words that meet the ratio cutoff in BOTH corpora, print result\n",
    "        cutoff_list = list()\n",
    "        candidate_words = list(wcount_one.keys()) + list(wcount_two.keys())\n",
    "        candidate_words = set(candidate_words)\n",
    "        for word in candidate_words :\n",
    "            if wcount_one[word] >= self.ratio_cutoff and wcount_two[word] >= self.ratio_cutoff :\n",
    "                cutoff_list.append(word)\n",
    "\n",
    "        cutoff_statement = f'There are {\"{:,}\".format(len(cutoff_list))} words that meet the usage cutoff of {self.ratio_cutoff} appearances in both Group 1 and Group 2'\n",
    "\n",
    "        # SECT3: CALCULATE METRICS ON WORDS IN CUTOFF_LIST\n",
    "        # Create \"metrics\", an intermediate dict to hold data that will be passed \n",
    "        # to different dictionaries in the final \"results\" dict of dicts output\n",
    "        metrics = defaultdict(list)\n",
    "\n",
    "        for word in cutoff_list:        \n",
    "            metrics[word].append(len([w for w in g1_clean if w==word]))  # word count in corpus_1\n",
    "            metrics[word].append(len([w for w in g2_clean if w==word]))  # word count in corpus_2\n",
    "            metrics[word].append(len([w for w in g1_clean if w==word])/g1_len)  # ratio of word count to corpus_1 length\n",
    "            metrics[word].append(len([w for w in g2_clean if w==word])/g2_len)  # ratio of word count to corpus_2 length \n",
    "\n",
    "        # Loop through through the defaultdict and append word ratio for the each word of the corpus\n",
    "        for word, nums in metrics.items() :\n",
    "            # Make sure any zero ratios are excluded\n",
    "            if (nums[2] * nums[3] > 0) :\n",
    "                metrics[word].append(nums[2]/nums[3])  # appends Corpus_1/Corpus_2 index\n",
    "                metrics[word].append(nums[3]/nums[2])  # appends Corpus_2/Corpus_1 index\n",
    "            else :\n",
    "                metrics[word].append(None)\n",
    "                metrics[word].append(None)\n",
    "\n",
    "\n",
    "        # SECT4: APPEND RESPECTIVE INDEXES (CORP1/CORP2 RATIO AND CORP2/CORP1 RATIO) TO \"METRICS\" DICT\n",
    "        one_v_two = defaultdict(list)\n",
    "        two_v_one = defaultdict(list)\n",
    "\n",
    "        # Append word/index pair in \"one_v_two\" dict\n",
    "        for word, nums in metrics.items() :\n",
    "            one_v_two[word].append(nums[4])\n",
    "\n",
    "        # Sort \"one_v_two\" down to just the \"num_words\" key:value pairs,\n",
    "        # sorted in descending order of index ratio.\n",
    "        one_v_two_sort = dict(sorted(one_v_two.items(), key = itemgetter(1), reverse = True)[:self.num_words])\n",
    "\n",
    "        # Lastly, round those values to two decimal points for readability    \n",
    "        for dict_value in one_v_two_sort :\n",
    "            for k, v in one_v_two_sort.items() :\n",
    "                one_v_two_sort[k] = [round(v,2) for v in one_v_two_sort[k]]\n",
    "\n",
    "        # Same process for \"two_v_one\"\n",
    "        # Append word/index pair in \"two_v_one\" dict\n",
    "        for word, nums in metrics.items() :\n",
    "            two_v_one[word].append(nums[5])\n",
    "\n",
    "        # Sort \"two_v_one\" down to just the \"num_words\" key:value pairs, \n",
    "        # sorted in descending order of index ratio.\n",
    "        two_v_one_sort = dict(sorted(two_v_one.items(), key = itemgetter(1), reverse = True)[:self.num_words])         \n",
    "\n",
    "        # Lastly, round those values to two decimal points for readability    \n",
    "        for dict_value in two_v_one_sort :\n",
    "            for k, v in two_v_one_sort.items() :\n",
    "                two_v_one_sort[k] = [round(v,2) for v in two_v_one_sort[k]]      \n",
    "\n",
    "\n",
    "        # SECT5: CALCULATE METRICS FOR \"CORP1\" AND \"CORP2\" KEYS IN \"RESULTS\" FINAL DICT\n",
    "        # Descriptive stats calcs for corpus_1\n",
    "        total_tokens_1 = len(g1_clean)\n",
    "        unique_tokens_1 = len(set(g1_clean))\n",
    "        word_len_1 = [len(w) for w in g1_clean]\n",
    "        avg_token_len_1 = np.mean(word_len_1)\n",
    "        lex_diversity_1 = len(set(g1_clean))/len(g1_clean)\n",
    "        top_n_1 = Counter(g1_clean).most_common(self.num_words)\n",
    "\n",
    "        # Descriptive stats calcs for corpus_2\n",
    "        total_tokens_2 = len(g2_clean)\n",
    "        unique_tokens_2 = len(set(g2_clean))\n",
    "        word_len_2 = [len(w) for w in g2_clean]\n",
    "        avg_token_len_2 = np.mean(word_len_2)\n",
    "        lex_diversity_2 = len(set(g2_clean))/len(g2_clean)\n",
    "        top_n_2 = Counter(g2_clean).most_common(self.num_words)\n",
    "\n",
    "\n",
    "        # SECT6: BRING EVERYTHING TOGETHER IN \"RESULTS\" FINAL DICT\n",
    "        results1 = {'Group_1': {'Number_of_tweets':group_1_tweets,\n",
    "                                'Number_of_words':total_tokens_1,\n",
    "                              'Unique_words':unique_tokens_1,\n",
    "                              'Avg _length':round(avg_token_len_1, 2),\n",
    "                              'Lexical_diversity':round(lex_diversity_1, 2),\n",
    "                              'Top_'+str(self.num_words):top_n_1},\n",
    "                   'Group_2': {'Number_of_tweets':group_2_tweets,\n",
    "                               'Number_of_words':total_tokens_2,\n",
    "                              'Unique_words':unique_tokens_2,\n",
    "                              'Avg_word_length':round(avg_token_len_2, 2),\n",
    "                              'Lexical_diversity':round(lex_diversity_2, 2),\n",
    "                              'Top_'+str(self.num_words):top_n_2},\n",
    "                   'Group1_vs_Group2': one_v_two_sort,\n",
    "                   'Group2_vs_Group1': two_v_one_sort,\n",
    "                   'cutoff_statement': cutoff_statement}\n",
    "\n",
    "        # Print elapsed time after function has run\n",
    "        end_time = datetime.datetime.now()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(elapsed_time)\n",
    "        return(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "50d57bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = re.compile(r\"\\bfuture\\b\")\n",
    "r1 = Lexicon(all_desc.apply(str), search_1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6f6c20ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus is 27108 total tweets\n",
      "The search string is re.compile('\\\\bfuture\\\\b')\n",
      "Words must appear 3 times in both corpora to be included in analysis\n",
      "Descriptive stats and comparisons for 10 words are returned\n"
     ]
    }
   ],
   "source": [
    "r1.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "44229747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.883474\n"
     ]
    }
   ],
   "source": [
    "round1 = r1.lex_expansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "734f5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 tweets:\n",
      "71\n",
      "Group 2 tweets:\n",
      "26,442\n",
      "There are 25 words that meet the usage cutoff of 3 appearances in both Group 1 and Group 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Group 1 tweets:')\n",
    "print(round1['Group_1']['Number_of_tweets'])\n",
    "print(f'Group 2 tweets:')\n",
    "print(round1['Group_2'][\"Number_of_tweets\"])\n",
    "print(round1[\"cutoff_statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0826e88e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Group 1 v Group 2:\n",
      "[('generations', [170.72]),\n",
      " ('Scotlandâ€™s', [56.91]),\n",
      " ('weâ€™re', [56.91]),\n",
      " ('current', [56.91]),\n",
      " ('wild', [34.14]),\n",
      " ('build', [26.56]),\n",
      " ('clean', [18.97]),\n",
      " ('better', [18.29]),\n",
      " ('healthy', [17.07]),\n",
      " ('Promoting', [17.07])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 1: Group 1 v Group 2:')\n",
    "pprint(sorted(round1['Group1_vs_Group2'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "05420cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Group 2 v Group 1:\n",
      "[('climate', [0.86]),\n",
      " ('people', [0.27]),\n",
      " ('health', [0.26]),\n",
      " ('global', [0.25]),\n",
      " ('world', [0.21]),\n",
      " ('together', [0.17]),\n",
      " ('planet', [0.16]),\n",
      " ('energy', [0.12]),\n",
      " ('you.', [0.11]),\n",
      " ('environment,', [0.1])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 1: Group 2 v Group 1:')\n",
    "pprint(sorted(round1['Group2_vs_Group1'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1dcbd",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "something SOMETHING\n",
    "<hr style=\"border:2px solid gray\"> </hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfe385",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = re.compile(r\"\\bclimate\\b\")\n",
    "r1 = Lexicon(db.description.apply(str), search_1, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round1 = r1.lex_expansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7720a58d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 tweets:\n",
      "1,539\n",
      "Group 2 tweets:\n",
      "336,432\n",
      "There are 1,129 words that meet the usage cutoff\n"
     ]
    }
   ],
   "source": [
    "print(f'Group 1 tweets:')\n",
    "print(x2['Group_1']['Number_of_tweets'])\n",
    "print(f'Group 2 tweets:')\n",
    "print(x2['Group_2'][\"Number_of_tweets\"])\n",
    "print(x2[\"cutoff_statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Round 2: Group 1 v Group 2:')\n",
    "pprint(sorted(x2['Group1_vs_Group2'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88b4c41d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 3: Group 2 v Group 1:\n",
      "[('#vapefam', [11.27]),\n",
      " ('#vape', [7.98]),\n",
      " ('pod', [6.98]),\n",
      " ('#vapelife', [6.39]),\n",
      " ('tank', [5.92]),\n",
      " ('addicted', [5.4]),\n",
      " ('#eliquid', [5.39]),\n",
      " ('#vaping', [5.02]),\n",
      " ('#vapelyfe', [4.89]),\n",
      " ('vapes', [4.78])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 2: Group 2 v Group 1:')\n",
    "pprint(sorted(x2['Group2_vs_Group1'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502f209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d300e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888e703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a99529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
