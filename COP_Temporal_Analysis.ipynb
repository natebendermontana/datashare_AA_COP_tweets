{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb69ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "\n",
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from my_api_keys import api_key, api_key_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c561cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2021, 10, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f4f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the Tweepy API\n",
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f5beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the search API only goes back 7 days\n",
    "date_end = '2021-10-31'\n",
    "search_words = \"#cop26 OR #cop26Glasgow -filter:retweets\"\n",
    "count=200\n",
    "\n",
    "# Notice the differences between searching tweets and users. \n",
    "tweets = tweepy.Cursor(api.search_tweets,\n",
    "                   # tweet_mode is defaulted to short, which only holds the first 140 characters of a Tweet.\n",
    "                   tweet_mode='extended',\n",
    "                   q=search_words,\n",
    "                   until=date_end,\n",
    "                   result_type='recent',\n",
    "                   lang='en'\n",
    "                                        ).items(count)\n",
    "#     if idx == 50:\n",
    "#         break\n",
    "    \n",
    "#     There's all sort of information you can get from Tweets\n",
    "#     Find more tweet objects here: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object\n",
    "#     print(item.user.id)\n",
    "#     print(item.user.screen_name)\n",
    "#     print(item.user.location)\n",
    "#     print(item.created_at)\n",
    "#     print(item.full_text)\n",
    "#     print('-'*40)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []\n",
    "for tweet in tweets:\n",
    "    # Filtering by date\n",
    "    if  datetime.date.today() == tweet.created_at.date():  \n",
    "        tweets_list.append([\n",
    "        item.user.id,\n",
    "        item.user.screen_name,\n",
    "        item.user.description,\n",
    "        item.user.location,\n",
    "        item.user.followers_count,\n",
    "        item.user.friends_count,    \n",
    "        item.created_at,\n",
    "        item.full_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11540867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ac51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapetweets(search_words, numtweets, numruns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try numruns times\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['user_id','screen_name','description','location','friends_count',\n",
    "           'followers_count','totaltweets','date_created','retweetcount','full_text','hashtags'])\n",
    "    \n",
    "    program_start = time.time()\n",
    "    for i in range(0, numruns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search_tweets, \n",
    "                               q=search_words, \n",
    "                               lang=\"en\", \n",
    "                               tweet_mode='extended'\n",
    "                              ).items(numtweets)\n",
    "        \n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        \n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "    \n",
    "        for tweet in tweet_list:\n",
    "            userid = tweet.user.id\n",
    "            username = tweet.user.screen_name\n",
    "            description = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            date_created = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            full_text = tweet.full_text\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [userid, username, description, location, following, followers, totaltweets,\n",
    "                         date_created, retweetcount, full_text, hashtags]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "\n",
    "#             except TweepError as err: \n",
    "#                 if err.code == 103: #if we get a rate limit error, go to sleep\n",
    "#                     print('sleeping, 900 seconds')\n",
    "#                     time.sleep(900)            \n",
    "            \n",
    "            # Run ended:\n",
    "            end_run = time.time()\n",
    "            duration_run = round((end_run-start_run)/60, 2)\n",
    "\n",
    "            print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "            print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "\n",
    "    # Once all runs have completed, save them to a single csv file:\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.date.today().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_cop_tweets.csv'\n",
    "    \n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrape is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8181fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 1 is 1\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 2\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 3\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 4\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 5\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 6\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 7\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 8\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 9\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 10\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 11\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 12\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 13\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 14\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 15\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 16\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 17\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 18\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 19\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 20\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 21\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 22\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 23\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 24\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 25\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 26\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 27\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 28\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 29\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 30\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 31\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 32\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 33\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 34\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 35\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 36\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 37\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 38\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 39\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 40\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 41\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 42\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 43\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 44\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 45\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 46\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 47\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 48\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 49\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 50\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 51\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 52\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 53\n",
      "time take for 1 run to complete is 0.04 mins\n",
      "no. of tweets scraped for run 1 is 54\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 55\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 56\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 57\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 58\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 59\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 60\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 61\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 62\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 63\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 64\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 65\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 66\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 67\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 68\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 69\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 70\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 71\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 72\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 73\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 74\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 75\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 76\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 77\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 78\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 79\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 80\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 81\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 82\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 83\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 84\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 85\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 86\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 87\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 88\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 89\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 90\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 91\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 92\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 93\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 94\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 95\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 96\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 97\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 98\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 99\n",
      "time take for 1 run to complete is 0.05 mins\n",
      "no. of tweets scraped for run 1 is 100\n",
      "time take for 1 run to complete is 0.05 mins\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/natebender/Desktop/Repo/text-mining/datashare_AA_COP_tweets/data/20211030_000000_cop_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-448af8a454b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscrapetweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-66f0a2e35d4c>\u001b[0m in \u001b[0;36mscrapetweets\u001b[0;34m(search_words, numtweets, numruns)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Store dataframe in csv with creation date timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mdb_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprogram_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/natebender/Desktop/Repo/text-mining/datashare_AA_COP_tweets/data/20211030_000000_cop_tweets.csv'"
     ]
    }
   ],
   "source": [
    "search_words = \"#cop26 OR #cop26Glasgow -filter:retweets\"\n",
    "numtweets=100\n",
    "numruns=1\n",
    "\n",
    "scrapetweets(search_words, numtweets, numruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f446ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc10412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebbbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['user_id','screen_name','description','location','followers_count',\n",
    "           'friends_count','date_created','full_text']\n",
    "\n",
    "\n",
    "# Descriptions with emoji or non-Roman letters can cause trouble. Encoding your .txt file in utf-8 will help\n",
    "with open(f'test_cop_tweets.txt','w', encoding='utf-8') as out_file:\n",
    "    out_file.write('\\t'.join(headers) + '\\n')\n",
    "\n",
    "    for tweet in tweets_list:\n",
    "\n",
    "        # For accounts set to private, we won't be able to get the description unless we follow them\n",
    "        # Putting in a try/except statement, we can get around this issue.\n",
    "        user_id = str(user.id).replace('\\t',' ').replace('\\n',' ')\n",
    "        screen_name = str(user.screen_name).replace('\\t',' ').replace('\\n',' ')\n",
    "        description = str(user.description).replace('\\t',' ').replace('\\n',' ')\n",
    "        location = str(user.location).replace('\\t',' ').replace('\\n',' ')        \n",
    "        followers_count = str(user.followers_count).replace('\\t',' ').replace('\\n',' ')        \n",
    "        friends_count = str(user.friends_count).replace('\\t',' ').replace('\\n',' ')\n",
    "        date_created = str(item.created_at).replace('\\t',' ').replace('\\n',' ')        \n",
    "        full_text = str(item.full_text).replace('\\t',' ').replace('\\n',' ')        \n",
    "        \n",
    "        outline = [user_id, screen_name, description, location, followers_count, friends_count,\n",
    "                  date_created, full_text]\n",
    "\n",
    "        out_file.write('\\t'.join([str(item) for item in outline]) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
