{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec647a0",
   "metadata": {},
   "source": [
    "sentiment analysis over time\n",
    "\n",
    "volume of tweets\n",
    "\n",
    "do group comparison of the first three days versus all other days of the convention. Then shift the window one day and analyze again. Over and over throughout the convention. \n",
    "\n",
    "maybe display the top five words by index throughout the convention. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f5f07",
   "metadata": {},
   "source": [
    "###  Issues to address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd2509",
   "metadata": {},
   "source": [
    "deal with carriage returns\n",
    "\n",
    "tab-separator\n",
    "\n",
    "deal with hashtags separators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808ac66c",
   "metadata": {},
   "source": [
    "### Import packages / setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb69ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from my_api_keys import api_key, api_key_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f4f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the Tweepy API\n",
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683e2b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Modifications to punctuation and sw lists\n",
    "\n",
    "punctuation = set(punctuation)\n",
    "punctuation.add(\"â€™\")\n",
    "\n",
    "sw2 = set(sw)\n",
    "addl = (\"|\",\"-\",\"/\",\"â€¢\",\"&\", \"&amp;\")\n",
    "sw2.update(addl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298fc430",
   "metadata": {},
   "source": [
    "####  Read daily tweets CSVs into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ae473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.DataFrame(columns = ['user_id','screen_name','description','location','friends_count',\n",
    "           'followers_count','totaltweets','date_created', 'tweet_id', 'retweetcount','full_text'])\n",
    "\n",
    "file_location = \"/Users/natebender/Desktop/Repo/text-mining/datashare_AA_COP_tweets/data/\"\n",
    "files = sorted(os.listdir(file_location))\n",
    "for idx, file in enumerate(files):\n",
    "    \n",
    "    data = \"\".join([file_location,file])\n",
    "    datafile = pd.read_csv(data)\n",
    "\n",
    "    db = db.append(datafile,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce513612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check desc stats on overall descriptions before splitting into groups\n",
    "def get_patterns(all_tweets) :\n",
    "\n",
    "    all_desc = all_tweets.dropna()\n",
    "    all_str = \" \".join(all_desc)    \n",
    "    clean = [w for w in all_str.split() if w.lower() not in sw2]\n",
    "    \n",
    "    # Calculate your statistics here\n",
    "    total_tokens = len(clean)\n",
    "    unique_tokens = len(set(clean))\n",
    "    clean_tok_len = [len(w) for w in clean]\n",
    "    avg_token_len = np.mean(clean_tok_len)\n",
    "    lex_diversity = len(set(clean))/len(clean)\n",
    "    top_n = Counter(clean).most_common(20)\n",
    "    \n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':round(avg_token_len,2),\n",
    "               'lexical_diversity':round(lex_diversity,2),\n",
    "               'Top_n':top_n}\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ea17d",
   "metadata": {},
   "source": [
    "### Desc stats on database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf7beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: 285,084 tweets\n",
      "Descriptive stats are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': 5523996,\n",
       " 'unique_tokens': 170153,\n",
       " 'avg_token_length': 7.82,\n",
       " 'lexical_diversity': 0.03,\n",
       " 'Top_n': [('#COP26', 227147),\n",
       "  ('climate', 57446),\n",
       "  ('#COP26Glasgow', 24133),\n",
       "  ('world', 20434),\n",
       "  ('#cop26', 18871),\n",
       "  ('need', 18435),\n",
       "  ('fossil', 17574),\n",
       "  ('change', 15756),\n",
       "  ('#Unknown', 15720),\n",
       "  ('leaders', 14933),\n",
       "  ('people', 13392),\n",
       "  ('#ClimateAction', 13153),\n",
       "  ('Climate', 12960),\n",
       "  ('Glasgow', 12833),\n",
       "  ('global', 11994),\n",
       "  ('action', 11760),\n",
       "  ('#ClimateCrisis', 11448),\n",
       "  ('#COP26.', 10836),\n",
       "  ('us', 10811),\n",
       "  ('like', 10464)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NA values from description column\n",
    "all_tweets = db.full_text.dropna()\n",
    "print(f'Database: {\"{:,}\".format(len(db.tweet_id))} tweets')\n",
    "#print(f'Date range: {db.date_created.min()} to {db.date_created.max()}')\n",
    "print(f'Descriptive stats are:')\n",
    "get_patterns(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b8c50",
   "metadata": {},
   "source": [
    "### Lexicon expansion function to analyze tweets by time series window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6bb55124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexicon_time :\n",
    "    def __init__(self, corpus, search_condition, num_words, ratio_cutoff):\n",
    "        self.corpus = list(corpus)\n",
    "        self.search_condition = search_condition\n",
    "        self.num_words = num_words\n",
    "        self.ratio_cutoff = ratio_cutoff\n",
    "        \n",
    "    def parameters(self) :\n",
    "        print(f'The corpus is {\"{:,}\".format(len(self.corpus))} total tweets')\n",
    "        print(f\"The search string is {self.search_condition}\")\n",
    "        print(f\"Words must appear {self.ratio_cutoff} times in both corpora to be included in analysis\")\n",
    "        print(f\"Output for {self.num_words} words is returned\")\n",
    "        \n",
    "    def lex_expansion(self) :\n",
    "\n",
    "        # Set starting marker for keeping track of how long the function runs\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        group_1 = self.search_condition\n",
    "\n",
    "        group_1 = list(group_1)\n",
    "        group_2 = [string for string in self.corpus if string not in group_1]\n",
    "\n",
    "        group_1_tweets = (\"{:,}\".format(len(group_1)))\n",
    "        group_2_tweets = (\"{:,}\".format(len(group_2)))    \n",
    "\n",
    "        # Tokenize the two groups by first turning them each into single large strings\n",
    "        g1_str = \" \".join(group_1)    \n",
    "        g2_str = \" \".join(group_2)\n",
    "\n",
    "        # Then splitting each into lists of strings. Once we've created groups, we no longer need to know\n",
    "        # which words came from which specific tweets, as long as we keep track of which group the \n",
    "        # words come from.\n",
    "        # We also tokenize and normalize at this stage. We want to remove stopwords, but retain numeric as numbers\n",
    "        # could be used in interesting words, hashtags, or accounts in a corpus of tweets. \n",
    "        g1_uclean = [w for w in g1_str.split()]\n",
    "        g2_uclean = [w for w in g2_str.split()]\n",
    "\n",
    "        g1_clean = [w for w in g1_str.split() if w.lower() not in sw2]\n",
    "        g2_clean = [w for w in g2_str.split()if w.lower() not in sw2]\n",
    "\n",
    "        g1_len = len(g1_clean)\n",
    "        g2_len = len(g2_clean)\n",
    "\n",
    "        # SECT2: CREATE \"CUTOFF_LIST\" LIST OF WORDS THAT MEET CUTOFF RATIO\n",
    "        # Create Counter dictionary of each corpus, used for determining words that meet cutoff ratio\n",
    "        wcount_one = Counter(g1_clean)\n",
    "        wcount_two = Counter(g2_clean)\n",
    "\n",
    "        # Create list of words that meet the ratio cutoff in BOTH corpora, print result\n",
    "        cutoff_list = list()\n",
    "        candidate_words = list(wcount_one.keys()) + list(wcount_two.keys())\n",
    "        candidate_words = set(candidate_words)\n",
    "        for word in candidate_words :\n",
    "            if wcount_one[word] >= self.ratio_cutoff and wcount_two[word] >= self.ratio_cutoff :\n",
    "                cutoff_list.append(word)\n",
    "\n",
    "        cutoff_statement = f'There are {\"{:,}\".format(len(cutoff_list))} words that meet the usage cutoff of {self.ratio_cutoff} appearances in both Group 1 and Group 2'\n",
    "\n",
    "        # SECT3: CALCULATE METRICS ON WORDS IN CUTOFF_LIST\n",
    "        # Create \"metrics\", an intermediate dict to hold data that will be passed \n",
    "        # to different dictionaries in the final \"results\" dict of dicts output\n",
    "        metrics = defaultdict(list)\n",
    "\n",
    "        for word in cutoff_list:        \n",
    "            metrics[word].append(len([w for w in g1_clean if w==word]))  # word count in corpus_1\n",
    "            metrics[word].append(len([w for w in g2_clean if w==word]))  # word count in corpus_2\n",
    "            metrics[word].append(len([w for w in g1_clean if w==word])/g1_len)  # ratio of word count to corpus_1 length\n",
    "            metrics[word].append(len([w for w in g2_clean if w==word])/g2_len)  # ratio of word count to corpus_2 length \n",
    "\n",
    "        # Loop through through the defaultdict and append word ratio for the each word of the corpus\n",
    "        for word, nums in metrics.items() :\n",
    "            # Make sure any zero ratios are excluded\n",
    "            if (nums[2] * nums[3] > 0) :\n",
    "                metrics[word].append(nums[2]/nums[3])  # appends Corpus_1/Corpus_2 index\n",
    "                metrics[word].append(nums[3]/nums[2])  # appends Corpus_2/Corpus_1 index\n",
    "            else :\n",
    "                metrics[word].append(None)\n",
    "                metrics[word].append(None)\n",
    "\n",
    "\n",
    "        # SECT4: APPEND RESPECTIVE INDEXES (CORP1/CORP2 RATIO AND CORP2/CORP1 RATIO) TO \"METRICS\" DICT\n",
    "        one_v_two = defaultdict(list)\n",
    "        two_v_one = defaultdict(list)\n",
    "\n",
    "        # Append word/index pair in \"one_v_two\" dict\n",
    "        for word, nums in metrics.items() :\n",
    "            one_v_two[word].append(nums[4])\n",
    "\n",
    "        # Sort \"one_v_two\" down to just the \"num_words\" key:value pairs,\n",
    "        # sorted in descending order of index ratio.\n",
    "        one_v_two_sort = dict(sorted(one_v_two.items(), key = itemgetter(1), reverse = True)[:self.num_words])\n",
    "\n",
    "        # Lastly, round those values to two decimal points for readability    \n",
    "        for dict_value in one_v_two_sort :\n",
    "            for k, v in one_v_two_sort.items() :\n",
    "                one_v_two_sort[k] = [round(v,2) for v in one_v_two_sort[k]]\n",
    "\n",
    "        # Same process for \"two_v_one\"\n",
    "        # Append word/index pair in \"two_v_one\" dict\n",
    "        for word, nums in metrics.items() :\n",
    "            two_v_one[word].append(nums[5])\n",
    "\n",
    "        # Sort \"two_v_one\" down to just the \"num_words\" key:value pairs, \n",
    "        # sorted in descending order of index ratio.\n",
    "        two_v_one_sort = dict(sorted(two_v_one.items(), key = itemgetter(1), reverse = True)[:self.num_words])         \n",
    "\n",
    "        # Lastly, round those values to two decimal points for readability    \n",
    "        for dict_value in two_v_one_sort :\n",
    "            for k, v in two_v_one_sort.items() :\n",
    "                two_v_one_sort[k] = [round(v,2) for v in two_v_one_sort[k]]      \n",
    "\n",
    "\n",
    "        # SECT5: CALCULATE METRICS FOR \"CORP1\" AND \"CORP2\" KEYS IN \"RESULTS\" FINAL DICT\n",
    "        # Descriptive stats calcs for corpus_1\n",
    "        total_tokens_1 = len(g1_clean)\n",
    "        unique_tokens_1 = len(set(g1_clean))\n",
    "        word_len_1 = [len(w) for w in g1_clean]\n",
    "        avg_token_len_1 = np.mean(word_len_1)\n",
    "        lex_diversity_1 = len(set(g1_clean))/len(g1_clean)\n",
    "        top_n_1 = Counter(g1_clean).most_common(self.num_words)\n",
    "\n",
    "        # Descriptive stats calcs for corpus_2\n",
    "        total_tokens_2 = len(g2_clean)\n",
    "        unique_tokens_2 = len(set(g2_clean))\n",
    "        word_len_2 = [len(w) for w in g2_clean]\n",
    "        avg_token_len_2 = np.mean(word_len_2)\n",
    "        lex_diversity_2 = len(set(g2_clean))/len(g2_clean)\n",
    "        top_n_2 = Counter(g2_clean).most_common(self.num_words)\n",
    "\n",
    "\n",
    "        # SECT6: BRING EVERYTHING TOGETHER IN \"RESULTS\" FINAL DICT\n",
    "        results1 = {'Group_1': {'Number_of_tweets':group_1_tweets,\n",
    "                                'Number_of_words':total_tokens_1,\n",
    "                              'Unique_words':unique_tokens_1,\n",
    "                              'Avg _length':round(avg_token_len_1, 2),\n",
    "                              'Lexical_diversity':round(lex_diversity_1, 2),\n",
    "                              'Top_'+str(self.num_words):top_n_1},\n",
    "                   'Group_2': {'Number_of_tweets':group_2_tweets,\n",
    "                               'Number_of_words':total_tokens_2,\n",
    "                              'Unique_words':unique_tokens_2,\n",
    "                              'Avg_word_length':round(avg_token_len_2, 2),\n",
    "                              'Lexical_diversity':round(lex_diversity_2, 2),\n",
    "                              'Top_'+str(self.num_words):top_n_2},\n",
    "                   'Group1_vs_Group2': one_v_two_sort,\n",
    "                   'Group2_vs_Group1': two_v_one_sort,\n",
    "                   'cutoff_statement': cutoff_statement}\n",
    "\n",
    "        # Print elapsed time after function has run\n",
    "        end_time = datetime.datetime.now()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(elapsed_time)\n",
    "        return(results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b50f5",
   "metadata": {},
   "source": [
    "#### Breaking groups by date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "971761eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07a75f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataframe to multiple 3-day windows\n",
    "\n",
    "w1_text = db.full_text[(db['date_created']>= \"2021-10-28\") & (db['date_created']< \"2021-10-31\")]\n",
    "w2_text = db.full_text[(db['date_created']>= \"2021-10-29\") & (db['date_created']< \"2021-11-01\")]\n",
    "w3_text = db.full_text[(db['date_created']>= \"2021-10-30\") & (db['date_created']< \"2021-11-02\")]\n",
    "w4_text = db.full_text[(db['date_created']>= \"2021-10-31\") & (db['date_created']< \"2021-11-03\")]\n",
    "w5_text = db.full_text[(db['date_created']>= \"2021-11-01\") & (db['date_created']< \"2021-11-04\")]\n",
    "w6_text = db.full_text[(db['date_created']>= \"2021-10-02\") & (db['date_created']< \"2021-11-05\")]\n",
    "w7_text = db.full_text[(db['date_created']>= \"2021-10-03\") & (db['date_created']< \"2021-11-06\")]\n",
    "w8_text = db.full_text[(db['date_created']>= \"2021-10-04\") & (db['date_created']< \"2021-11-07\")]\n",
    "w9_text = db.full_text[(db['date_created']>= \"2021-10-05\") & (db['date_created']< \"2021-11-08\")]\n",
    "# w10_text = db.full_text[(db['date_created']>= \"2021-10-06\") & (db['date_created']< \"2021-11-09\")]\n",
    "# w11_text = db.full_text[(db['date_created']>= \"2021-10-07\") & (db['date_created']< \"2021-11-10\")]\n",
    "# w12_text = db.full_text[(db['date_created']>= \"2021-10-08\") & (db['date_created']< \"2021-11-11\")]\n",
    "# w13_text = db.full_text[(db['date_created']>= \"2021-10-09\") & (db['date_created']< \"2021-11-12\")]\n",
    "# w14_text = db.full_text[(db['date_created']>= \"2021-10-10\") & (db['date_created']< \"2021-11-13\")]\n",
    "# w15_text = db.full_text[(db['date_created']>= \"2021-10-11\") & (db['date_created']< \"2021-11-14\")]\n",
    "# w16_text = db.full_text[(db['date_created']>= \"2021-10-12\") & (db['date_created']< \"2021-11-15\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "943f6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:12:45.664241\n"
     ]
    }
   ],
   "source": [
    "window1 = Lexicon_time(all_tweets, w1_text, 20, 10)\n",
    "win1 = window1.lex_expansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "af8b37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 tweets:\n",
      "45,000\n",
      "Group 2 tweets:\n",
      "133,968\n",
      "There are 8,436 words that meet the usage cutoff of 10 appearances in both Group 1 and Group 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Group 1 tweets:')\n",
    "print(win1['Group_1']['Number_of_tweets'])\n",
    "print(f'Group 2 tweets:')\n",
    "print(win1['Group_2'][\"Number_of_tweets\"])\n",
    "print(win1[\"cutoff_statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a690b677",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Group 1 v Group 2:\n",
      "[('#LetsGetReal', [230.4]),\n",
      " ('#XiJinping', [132.87]),\n",
      " ('Empty', [119.07]),\n",
      " ('#ScottishIndependence14', [95.84]),\n",
      " ('@Plant_Treaty', [91.97]),\n",
      " ('endorse', [76.67]),\n",
      " ('#EndFossilFinance', [69.7]),\n",
      " ('prepares', [66.8]),\n",
      " ('fuels?', [60.02]),\n",
      " ('act!', [57.36]),\n",
      " ('Pls', [55.76]),\n",
      " ('help!', [53.73]),\n",
      " ('option!', [52.28]),\n",
      " ('factory', [50.53]),\n",
      " ('NOW.', [46.1]),\n",
      " ('vague', [42.69]),\n",
      " ('REAL', [42.35]),\n",
      " ('pledges.', [39.37]),\n",
      " ('@s_guilbeault:', [36.59]),\n",
      " ('accepting', [36.3])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 1: Group 1 v Group 2:')\n",
    "pprint(sorted(win1['Group1_vs_Group2'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "da58fafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:01:58.618177\n"
     ]
    }
   ],
   "source": [
    "window2 = Lexicon_time(all_tweets, w2_text, 20, 10)\n",
    "win2 = window2.lex_expansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "50be2824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 tweets:\n",
      "45,000\n",
      "Group 2 tweets:\n",
      "133,200\n",
      "There are 8,440 words that meet the usage cutoff of 10 appearances in both Group 1 and Group 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Group 1 tweets:')\n",
    "print(win2['Group_1']['Number_of_tweets'])\n",
    "print(f'Group 2 tweets:')\n",
    "print(win2['Group_2'][\"Number_of_tweets\"])\n",
    "print(win2[\"cutoff_statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dc01e74b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2: Group 1 v Group 2:\n",
      "[('#ScottishIndependence14', [202.28]),\n",
      " ('#BanTrophyHunting', [143.4]),\n",
      " ('fuels?', [99.63]),\n",
      " ('#G20RomeSummit', [78.5]),\n",
      " ('prepares', [72.46]),\n",
      " ('#EndFossilFinance', [56.61]),\n",
      " ('option!', [54.34]),\n",
      " ('#PlantBasedTreaty', [38.12]),\n",
      " ('accepting', [37.74]),\n",
      " ('endorse', [34.58]),\n",
      " ('@HarjitSajjan', [33.21]),\n",
      " ('privatised', [33.21]),\n",
      " ('âœï¸', [31.87]),\n",
      " ('#Shorts', [31.7]),\n",
      " ('@Plant_Treaty', [29.85]),\n",
      " ('Peter', [29.28]),\n",
      " ('counts.', [28.68]),\n",
      " ('ðŸ˜·/', [28.68]),\n",
      " ('talks:', [28.68]),\n",
      " ('#Insiders', [28.18])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 2: Group 1 v Group 2:')\n",
    "pprint(sorted(win2['Group1_vs_Group2'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2ee98d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sorted(win2['Group1_vs_Group2'].items(), key = itemgetter(1), reverse = True)\n",
    "test2 = test[0][1][0]  # example of how to access the value for \"scottishIndependence14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45315c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b2852415",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-a50b508e95dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwindow3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLexicon_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw3_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwin3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex_expansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-159-b84a9c2d894a>\u001b[0m in \u001b[0;36mlex_expansion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg2_clean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# word count in corpus_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg1_clean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mg1_len\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ratio of word count to corpus_1 length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg2_clean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mg2_len\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ratio of word count to corpus_2 length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Loop through through the defaultdict and append word ratio for the each word of the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-b84a9c2d894a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg2_clean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# word count in corpus_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg1_clean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mg1_len\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ratio of word count to corpus_1 length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg2_clean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mg2_len\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ratio of word count to corpus_2 length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Loop through through the defaultdict and append word ratio for the each word of the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "window3 = Lexicon_time(all_tweets, w3_text, 20, 10)\n",
    "win3 = window3.lex_expansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ccd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Group 1 tweets:')\n",
    "print(win3['Group_1']['Number_of_tweets'])\n",
    "print(f'Group 2 tweets:')\n",
    "print(win3['Group_2'][\"Number_of_tweets\"])\n",
    "print(win3[\"cutoff_statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f94e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Round 3: Group 1 v Group 2:')\n",
    "pprint(sorted(win3['Group1_vs_Group2'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6ee44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "48276c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plantbasedtreaty    \n",
    "x = [1, 2, 3]\n",
    "y = [240.71, 10, 0]\n",
    "plt.plot(x, y)\n",
    "  \n",
    "# Letsgetreal    \n",
    "x1 = [1, 2, 3]\n",
    "y1 = [265.23, 150, 400] \n",
    "# second plot with x1 and y1 data\n",
    "plt.plot(x1, y1)\n",
    "  \n",
    "# example    \n",
    "x2 = [1, 2, 3]\n",
    "y2 = [0, 0, 1050] \n",
    "# second plot with x1 and y1 data\n",
    "plt.plot(x2, y2)    \n",
    "\n",
    "plt.xlabel(\"Rolling 3-day Window\")\n",
    "plt.ylabel(\"Usage Index\")\n",
    "plt.title('#COP26 Tweets')\n",
    "plt.xticks([1, 2, 3], [\"10/28\\n10/31\", \"10/29\\n11/01\", \"10/30\\n11/02\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31974e0",
   "metadata": {},
   "source": [
    "- Figure out how to populate the x and y axes automatically from the dictionary of results for each iteration. \n",
    "- Add labels of the word names\n",
    "- Add slider bar for the x axis to it can be interactive? User could slide the bar back and forth to more easily see how the usage changed throughout the conference. Bold items that correspond to the window the slider is on. \n",
    "- Add add'l layer of interactivity (toggle on/off?) that brings in info about what was being emphasized at different point of the conference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d54ca2",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42968887",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba4238",
   "metadata": {},
   "source": [
    "# Lexicon expansion function to analyze tweets by group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fca8d",
   "metadata": {},
   "source": [
    "#### ROUND 1\n",
    "<hr style=\"border:2px solid gray\"> </hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d7ebe",
   "metadata": {},
   "source": [
    "#### Breaking groups by search terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f8b65",
   "metadata": {},
   "source": [
    "r\"#[copCOP]+26\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexicon :\n",
    "    def __init__(self, corpus, search_condition, num_words, ratio_cutoff):\n",
    "        self.corpus = list(corpus)\n",
    "        self.search_condition = search_condition\n",
    "        self.num_words = num_words\n",
    "        self.ratio_cutoff = ratio_cutoff\n",
    "        \n",
    "    def parameters(self) :\n",
    "        print(f'The corpus is {\"{:,}\".format(len(self.corpus))} total tweets')\n",
    "        print(f\"The search string is {self.search_condition}\")\n",
    "        print(f\"Words must appear {self.ratio_cutoff} times in both corpora to be included in analysis\")\n",
    "        print(f\"Output for {self.num_words} words is returned\")\n",
    "        \n",
    "    def lex_expansion(self) :\n",
    "\n",
    "        # Set starting marker for keeping track of how long the function runs\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        group_1 = []\n",
    "\n",
    "        for string in self.corpus :\n",
    "            if self.search_condition.search(string) :\n",
    "                group_1.append(string)\n",
    "\n",
    "        full_group_1 = list(group_1)        \n",
    "        group_1 = list(set(group_1))\n",
    "        group_2 = [string for string in self.corpus if string not in group_1]\n",
    "\n",
    "        full_group_1_tweets = (\"{:,}\".format(len(full_group_1)))\n",
    "        group_1_tweets = (\"{:,}\".format(len(group_1)))\n",
    "        group_2_tweets = (\"{:,}\".format(len(group_2)))    \n",
    "\n",
    "        # Tokenize the two groups by first turning them each into single large strings\n",
    "        g1_str = \" \".join(group_1)    \n",
    "        g2_str = \" \".join(group_2)\n",
    "\n",
    "        # Then splitting each into lists of strings. Once we've created groups, we no longer need to know\n",
    "        # which words came from which specific tweets, as long as we keep track of which group the \n",
    "        # words come from.\n",
    "        # We also tokenize and normalize at this stage. We want to remove stopwords, but retain numeric as numbers\n",
    "        # could be used in interesting words, hashtags, or accounts in a corpus of tweets. \n",
    "        g1_uclean = [w for w in g1_str.split()]\n",
    "        g2_uclean = [w for w in g2_str.split()]\n",
    "\n",
    "        g1_clean = [w for w in g1_str.split() if w.lower() not in sw2]\n",
    "        g2_clean = [w for w in g2_str.split()if w.lower() not in sw2]\n",
    "\n",
    "        g1_len = len(g1_clean)\n",
    "        g2_len = len(g2_clean)\n",
    "\n",
    "        # SECT2: CREATE \"CUTOFF_LIST\" LIST OF WORDS THAT MEET CUTOFF RATIO\n",
    "        # Create Counter dictionary of each corpus, used for determining words that meet cutoff ratio\n",
    "        wcount_one = Counter(g1_clean)\n",
    "        wcount_two = Counter(g2_clean)\n",
    "\n",
    "        # Create list of words that meet the ratio cutoff in BOTH corpora, print result\n",
    "        cutoff_list = list()\n",
    "        candidate_words = list(wcount_one.keys()) + list(wcount_two.keys())\n",
    "        candidate_words = set(candidate_words)\n",
    "        for word in candidate_words :\n",
    "            if wcount_one[word] >= self.ratio_cutoff and wcount_two[word] >= self.ratio_cutoff :\n",
    "                cutoff_list.append(word)\n",
    "\n",
    "        cutoff_statement = f'There are {\"{:,}\".format(len(cutoff_list))} words that meet the usage cutoff of {self.ratio_cutoff} appearances in both Group 1 and Group 2'\n",
    "\n",
    "        # SECT3: CALCULATE METRICS ON WORDS IN CUTOFF_LIST\n",
    "        # Create \"metrics\", an intermediate dict to hold data that will be passed \n",
    "        # to different dictionaries in the final \"results\" dict of dicts output\n",
    "        metrics = defaultdict(list)\n",
    "\n",
    "        for word in cutoff_list:        \n",
    "            metrics[word].append(len([w for w in g1_clean if w==word]))  # word count in corpus_1\n",
    "            metrics[word].append(len([w for w in g2_clean if w==word]))  # word count in corpus_2\n",
    "            metrics[word].append(len([w for w in g1_clean if w==word])/g1_len)  # ratio of word count to corpus_1 length\n",
    "            metrics[word].append(len([w for w in g2_clean if w==word])/g2_len)  # ratio of word count to corpus_2 length \n",
    "\n",
    "        # Loop through through the defaultdict and append word ratio for the each word of the corpus\n",
    "        for word, nums in metrics.items() :\n",
    "            # Make sure any zero ratios are excluded\n",
    "            if (nums[2] * nums[3] > 0) :\n",
    "                metrics[word].append(nums[2]/nums[3])  # appends Corpus_1/Corpus_2 index\n",
    "                metrics[word].append(nums[3]/nums[2])  # appends Corpus_2/Corpus_1 index\n",
    "            else :\n",
    "                metrics[word].append(None)\n",
    "                metrics[word].append(None)\n",
    "\n",
    "\n",
    "        # SECT4: APPEND RESPECTIVE INDEXES (CORP1/CORP2 RATIO AND CORP2/CORP1 RATIO) TO \"METRICS\" DICT\n",
    "        one_v_two = defaultdict(list)\n",
    "        two_v_one = defaultdict(list)\n",
    "\n",
    "        # Append word/index pair in \"one_v_two\" dict\n",
    "        for word, nums in metrics.items() :\n",
    "            one_v_two[word].append(nums[4])\n",
    "\n",
    "        # Sort \"one_v_two\" down to just the \"num_words\" key:value pairs,\n",
    "        # sorted in descending order of index ratio.\n",
    "        one_v_two_sort = dict(sorted(one_v_two.items(), key = itemgetter(1), reverse = True)[:self.num_words])\n",
    "\n",
    "        # Lastly, round those values to two decimal points for readability    \n",
    "        for dict_value in one_v_two_sort :\n",
    "            for k, v in one_v_two_sort.items() :\n",
    "                one_v_two_sort[k] = [round(v,2) for v in one_v_two_sort[k]]\n",
    "\n",
    "        # Same process for \"two_v_one\"\n",
    "        # Append word/index pair in \"two_v_one\" dict\n",
    "        for word, nums in metrics.items() :\n",
    "            two_v_one[word].append(nums[5])\n",
    "\n",
    "        # Sort \"two_v_one\" down to just the \"num_words\" key:value pairs, \n",
    "        # sorted in descending order of index ratio.\n",
    "        two_v_one_sort = dict(sorted(two_v_one.items(), key = itemgetter(1), reverse = True)[:self.num_words])         \n",
    "\n",
    "        # Lastly, round those values to two decimal points for readability    \n",
    "        for dict_value in two_v_one_sort :\n",
    "            for k, v in two_v_one_sort.items() :\n",
    "                two_v_one_sort[k] = [round(v,2) for v in two_v_one_sort[k]]      \n",
    "\n",
    "\n",
    "        # SECT5: CALCULATE METRICS FOR \"CORP1\" AND \"CORP2\" KEYS IN \"RESULTS\" FINAL DICT\n",
    "        # Descriptive stats calcs for corpus_1\n",
    "        total_tokens_1 = len(g1_clean)\n",
    "        unique_tokens_1 = len(set(g1_clean))\n",
    "        word_len_1 = [len(w) for w in g1_clean]\n",
    "        avg_token_len_1 = np.mean(word_len_1)\n",
    "        lex_diversity_1 = len(set(g1_clean))/len(g1_clean)\n",
    "        top_n_1 = Counter(g1_clean).most_common(self.num_words)\n",
    "\n",
    "        # Descriptive stats calcs for corpus_2\n",
    "        total_tokens_2 = len(g2_clean)\n",
    "        unique_tokens_2 = len(set(g2_clean))\n",
    "        word_len_2 = [len(w) for w in g2_clean]\n",
    "        avg_token_len_2 = np.mean(word_len_2)\n",
    "        lex_diversity_2 = len(set(g2_clean))/len(g2_clean)\n",
    "        top_n_2 = Counter(g2_clean).most_common(self.num_words)\n",
    "\n",
    "\n",
    "        # SECT6: BRING EVERYTHING TOGETHER IN \"RESULTS\" FINAL DICT\n",
    "        results1 = {'Group_1': {'Full_tweets':full_group_1_tweets,\n",
    "                                'Number_of_tweets':group_1_tweets,\n",
    "                                'Number_of_words':total_tokens_1,\n",
    "                              'Unique_words':unique_tokens_1,\n",
    "                              'Avg _length':round(avg_token_len_1, 2),\n",
    "                              'Lexical_diversity':round(lex_diversity_1, 2),\n",
    "                              'Top_'+str(self.num_words):top_n_1},\n",
    "                   'Group_2': {'Number_of_tweets':group_2_tweets,\n",
    "                               'Number_of_words':total_tokens_2,\n",
    "                              'Unique_words':unique_tokens_2,\n",
    "                              'Avg_word_length':round(avg_token_len_2, 2),\n",
    "                              'Lexical_diversity':round(lex_diversity_2, 2),\n",
    "                              'Top_'+str(self.num_words):top_n_2},\n",
    "                   'Group1_vs_Group2': one_v_two_sort,\n",
    "                   'Group2_vs_Group1': two_v_one_sort,\n",
    "                   'cutoff_statement': cutoff_statement}\n",
    "\n",
    "        # Print elapsed time after function has run\n",
    "        end_time = datetime.datetime.now()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(elapsed_time)\n",
    "        return(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50d57bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = re.compile(r\"activis[mt]\\w?\") # match either uppercase #COP26 or lowercase #cop26\n",
    "r1 = Lexicon(all_tweets, search_1, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f6c20ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus is 150,000 total tweets\n",
      "The search string is re.compile('activis[mt]\\\\w?')\n",
      "Words must appear 5 times in both corpora to be included in analysis\n",
      "Output for 10 words is returned\n"
     ]
    }
   ],
   "source": [
    "r1.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44229747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:02.711628\n"
     ]
    }
   ],
   "source": [
    "round1 = r1.lex_expansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "734f5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 tweets:\n",
      "3,432\n",
      "572\n",
      "Group 2 tweets:\n",
      "146,568\n",
      "There are 253 words that meet the usage cutoff of 5 appearances in both Group 1 and Group 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Group 1 tweets:')\n",
    "print(round1['Group_1']['Full_tweets'])\n",
    "print(round1['Group_1']['Number_of_tweets'])\n",
    "print(f'Group 2 tweets:')\n",
    "print(round1['Group_2'][\"Number_of_tweets\"])\n",
    "print(round1[\"cutoff_statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0826e88e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Group 1 v Group 2:\n",
      "[('offsets,', [3047.26]),\n",
      " ('.@s_guilbeault', [2978.0]),\n",
      " ('defied', [1662.14]),\n",
      " ('fuels!', [1540.94]),\n",
      " ('@GreenpeaceCA', [1540.94]),\n",
      " ('heights', [1540.94]),\n",
      " ('tables', [727.19]),\n",
      " ('climate-safe', [623.3]),\n",
      " ('sailing', [484.79]),\n",
      " ('Bangladesh', [332.43])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 1: Group 1 v Group 2:')\n",
    "pprint(sorted(round1['Group1_vs_Group2'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05420cf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Group 2 v Group 1:\n",
      "[('one', [5.11]),\n",
      " ('#ClimateCrisis', [4.0]),\n",
      " ('#COP26Glasgow', [3.54]),\n",
      " ('carbon', [3.07]),\n",
      " ('#ClimateEmergency', [3.05]),\n",
      " ('via', [2.79]),\n",
      " ('need', [2.67]),\n",
      " ('know', [2.6]),\n",
      " ('get', [2.55]),\n",
      " ('planet', [2.47])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 1: Group 2 v Group 1:')\n",
    "pprint(sorted(round1['Group2_vs_Group1'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1dcbd",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### ROUND 2\n",
    "<hr style=\"border:2px solid gray\"> </hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31841d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_2 = re.compile(r\"activis[mt]\\w?|\\bdefied|\\boffsets?\\b\") # match either uppercase #COP26 or lowercase #cop26\n",
    "r2 = Lexicon(all_tweets, search_2, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77e35f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus is 150,000 total tweets\n",
      "The search string is re.compile('activis[mt]\\\\w?|\\\\bdefied|\\\\boffsets?\\\\b')\n",
      "Words must appear 5 times in both corpora to be included in analysis\n",
      "Output for 10 words is returned\n"
     ]
    }
   ],
   "source": [
    "r2.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa155a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:54.589681\n"
     ]
    }
   ],
   "source": [
    "round2 = r2.lex_expansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b86cadee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 2 tweets:\n",
      "3,732\n",
      "611\n",
      "Group 2 tweets:\n",
      "146,268\n",
      "There are 268 words that meet the usage cutoff of 5 appearances in both Group 1 and Group 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Group 2 tweets:')\n",
    "print(round2['Group_1']['Full_tweets'])\n",
    "print(round2['Group_1']['Number_of_tweets'])\n",
    "print(f'Group 2 tweets:')\n",
    "print(round2['Group_2'][\"Number_of_tweets\"])\n",
    "print(round2[\"cutoff_statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f4278bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2: Group 1 v Group 2:\n",
      "[('.@s_guilbeault', [2798.59]),\n",
      " ('fuels!', [1448.11]),\n",
      " ('@GreenpeaceCA', [1448.11]),\n",
      " ('heights', [1448.11]),\n",
      " ('tables', [683.38]),\n",
      " ('climate-safe', [585.75]),\n",
      " ('sailing', [455.58]),\n",
      " ('Bangladesh', [312.4]),\n",
      " ('YES', [286.37]),\n",
      " ('odds', [260.33])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 2: Group 1 v Group 2:')\n",
    "pprint(sorted(round2['Group1_vs_Group2'].items(), key = itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab98f97a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2: Group 2 v Group 1:\n",
      "[('one', [5.44]),\n",
      " ('@COP26', [3.46]),\n",
      " ('#COP26Glasgow', [3.4]),\n",
      " ('#ClimateCrisis', [3.31]),\n",
      " ('#ClimateEmergency', [3.25]),\n",
      " ('emissions', [3.03]),\n",
      " ('know', [2.77]),\n",
      " ('get', [2.71]),\n",
      " ('end', [2.68]),\n",
      " ('Scotland', [2.51])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Round 2: Group 2 v Group 1:')\n",
    "pprint(sorted(round2['Group2_vs_Group1'].items(), key = itemgetter(1), reverse = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
